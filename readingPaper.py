import arxiv
import pymupdf4llm
from openai import OpenAI
import os
import re
import time

# ================= é…ç½®åŒºåŸŸ =================
# 1. å¡«å…¥ä½ çš„ DeepSeek API Key
API_KEY = "sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx" 

# 2. è¾“å…¥å’Œè¾“å‡ºæ–‡ä»¶è®¾ç½®
INPUT_FILE = "papers.txt"         # å­˜æ”¾ ArXiv é“¾æ¥çš„æ–‡ä»¶ (ä¸€è¡Œä¸€ä¸ª)
OUTPUT_DIR = "paper_notes"        # ç¬”è®°ä¿å­˜ç›®å½•

# 3. å­—ç¬¦æ•°é™åˆ¶ (20ä¸‡å­—ç¬¦çº¦ç­‰äº 50k Tokensï¼Œè¶³å¤Ÿè¦†ç›– 30-40 é¡µçš„ ApJ è®ºæ–‡)
MAX_CHARS = 200000                 
# ===========================================

client = OpenAI(api_key=API_KEY, base_url="https://api.deepseek.com")

def extract_arxiv_id(url):
    """ä»é“¾æ¥ä¸­æå– ArXiv ID"""
    match = re.search(r'(\d{4}\.\d{4,5})', url)
    return match.group(1) if match else None

def strip_references(md_text):
    """
    å°è¯•åˆ‡é™¤å‚è€ƒæ–‡çŒ®éƒ¨åˆ†ä»¥èŠ‚çœ Tokenã€‚
    pymupdf4llm è½¬æ¢çš„ Markdown é€šå¸¸ä¼šå°† References ä½œä¸ºä¸€çº§æˆ–äºŒçº§æ ‡é¢˜ã€‚
    """
    # å¸¸è§çš„å‚è€ƒæ–‡çŒ®æ ‡é¢˜æ­£åˆ™åŒ¹é…
    # åŒ¹é…è¡Œé¦–çš„ # References, ## Bibliography ç­‰
    patterns = [
        r'\n#+\s*References', 
        r'\n#+\s*Bibliography', 
        r'\n#+\s*LITERATURE CITED'
    ]
    
    for pattern in patterns:
        # æœç´¢è¿™äº›æ ‡é¢˜çš„ä½ç½®
        matches = list(re.finditer(pattern, md_text, re.IGNORECASE))
        if matches:
            # æ‰¾åˆ°æœ€åä¸€ä¸ªåŒ¹é…é¡¹ï¼ˆé˜²æ­¢ç›®å½•ä¸­å‡ºç° References å­—æ ·è¯¯åˆ‡ï¼‰
            # é€šå¸¸å‚è€ƒæ–‡çŒ®åœ¨æ–‡ç« æœ€åï¼Œæ‰€ä»¥å–æœ€åä¸€ä¸ªåŒ¹é…æ˜¯æ¯”è¾ƒå®‰å…¨çš„
            last_match = matches[-1]
            print(f"âœ‚ï¸  æ£€æµ‹åˆ°å‚è€ƒæ–‡çŒ®éƒ¨åˆ†ï¼Œå·²åˆ‡é™¤ (ä½ç½®: {last_match.start()}/{len(md_text)})")
            return md_text[:last_match.start()]
            
    return md_text

def get_paper_content(arxiv_id):
    """ä¸‹è½½è®ºæ–‡å¹¶è½¬æ¢ä¸º Markdown"""
    print(f"â¬‡ï¸  æ­£åœ¨è·å–è®ºæ–‡å…ƒæ•°æ®: {arxiv_id}...")
    try:
        search = arxiv.Search(id_list=[arxiv_id])
        paper = next(search.results())
    except Exception as e:
        raise Exception(f"ArXiv ä¸‹è½½å¤±è´¥: {e}")
    
    pdf_filename = f"{arxiv_id}.pdf"
    
    # ä¸‹è½½ PDF
    if not os.path.exists(pdf_filename):
        print(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½ PDF...")
        paper.download_pdf(filename=pdf_filename)
    
    print(f"ğŸ“– æ­£åœ¨è§£æ PDF (ä¿ç•™ LaTeX å…¬å¼)...")
    try:
        # è½¬æ¢ä¸º Markdown
        md_text = pymupdf4llm.to_markdown(pdf_filename)
        
        # åˆ‡é™¤å‚è€ƒæ–‡çŒ®
        md_text = strip_references(md_text)
        
    except Exception as e:
        if os.path.exists(pdf_filename):
            os.remove(pdf_filename)
        raise Exception(f"PDF è§£æå¤±è´¥: {e}")
    
    # æ¸…ç†ä¸´æ—¶ PDF
    if os.path.exists(pdf_filename):
        os.remove(pdf_filename)
    
    return paper.title, md_text

def analyze_with_deepseek(title, content):
    """è°ƒç”¨ DeepSeek è¿›è¡Œæ·±åº¦æ€»ç»“"""
    print(f"ğŸ¤– DeepSeek æ­£åœ¨é˜…è¯»å¹¶åˆ†æ: {title}...")
    
    # æˆªæ–­ä»¥é˜²ä¸‡ä¸€ï¼Œè™½ç„¶20wé€šå¸¸å¤Ÿç”¨
    truncated_content = content[:MAX_CHARS]
    if len(content) > MAX_CHARS:
        print(f"âš ï¸  æ–‡ç« æé•¿ï¼Œå·²æˆªå–å‰ {MAX_CHARS} å­—ç¬¦")

    system_prompt = """
    ä½ æ˜¯ä¸€ä½èµ„æ·±çš„å¤©ä½“ç‰©ç†å­¦ç ”ç©¶å‘˜ã€‚è¯·é˜…è¯»ç”¨æˆ·æä¾›çš„è®ºæ–‡æ­£æ–‡ï¼ˆMarkdownæ ¼å¼ï¼‰ã€‚
    
    ã€ä»»åŠ¡æŒ‡ä»¤ã€‘
    1. é¦–å…ˆï¼Œè¯·åˆ¤æ–­è¿™ç¯‡è®ºæ–‡çš„ä¸»è¦å±æ€§ï¼ˆå•ä¸€æˆ–æ··åˆï¼‰ï¼š
       - **æ•°å€¼æ¨¡æ‹Ÿ (Numerical Simulation)**
       - **å¤©æ–‡è§‚æµ‹ (Observational Astronomy)**
       - **ç†è®ºæ¨å¯¼ (Theoretical Astrophysics)**
    
    2. è¯·ä¸¥æ ¼æŒ‰ç…§ä¸‹æ–¹ç»“æ„ç”Ÿæˆä¸­æ–‡é˜…è¯»æŠ¥å‘Šã€‚
    
    3. **å…³é”®è¦æ±‚**ï¼š
       - **ä¿ç•™ LaTeX å…¬å¼**ï¼šå‡¡æ˜¯æ¶‰åŠç‰©ç†é‡ï¼ˆå¦‚ $\dot{M}$, $\Sigma_{gas}$, $\alpha_{vir}$ï¼‰å¿…é¡»ä¿ç•™åŸæ ¼å¼ã€‚
       - **å®šé‡ä¼˜å…ˆ**ï¼šä¸è¦åªè¯´"ç»“æœå¢åŠ "ï¼Œè¦è¯´"å¢åŠ äº†çº¦ 3 å€"æˆ–"å¹‚å¾‹æŒ‡æ•°ä¸º -2.5"ã€‚
       - **ä»£ç ä¸ç»†èŠ‚**ï¼šå¯¹äºæ¨¡æ‹Ÿï¼Œå¿…é¡»æŒ‡å‡ºä½¿ç”¨çš„ Code (e.g., ORION, ATHENA) å’Œå…³é”®ç®—æ³•ã€‚
    
    ã€è¾“å‡ºç»“æ„ã€‘
    
    ### 1. ç ”ç©¶èƒŒæ™¯ä¸åŠ¨æœº (Context & Motivation)
    - ç ”ç©¶å¯¹è±¡ï¼ˆå¦‚ï¼šåŸæ’æ˜Ÿç›˜ã€åˆ†å­äº‘ã€æ˜Ÿç³»åé¦ˆï¼‰ã€‚
    - è¯•å›¾è§£å†³çš„å…·ä½“ç‰©ç†å¼ åŠ› (Tension) æˆ– è§‚æµ‹/ç†è®º çš„ç¼ºå¤±ç¯èŠ‚ã€‚
    
    ### 2. ç ”ç©¶æ–¹æ³• (Methodology)
    *(è¯·æ ¹æ®è®ºæ–‡ç±»å‹æ™ºèƒ½è°ƒæ•´é‡ç‚¹)*
    - **[æ¨¡æ‹Ÿ]**ï¼šä»£ç  (Code)ã€ç®—æ³• (MHD/Hydro/PIC)ã€åˆ†è¾¨ç‡ (Resolution)ã€åˆå§‹æ¡ä»¶ (IC) å’Œ ç‰©ç†æ¨¡å— (Physics modules)ã€‚
    - **[è§‚æµ‹]**ï¼šæœ›è¿œé•œ (Telescope)ã€è§‚æµ‹æ³¢æ®µ (Wavelength)ã€ç›®æ ‡æº (Target)ã€çµæ•åº¦/æ³¢æŸå¤§å° (Sensitivity/Beam)ã€‚
    - **[ç†è®º]**ï¼šæ ¸å¿ƒå‡è®¾ (Assumptions)ã€æ§åˆ¶æ–¹ç¨‹ (Governing Equations)ã€é€‚ç”¨èŒƒå›´ (Regime)ã€‚
    
    ### 3. ä¸»è¦ç»“æœ (Key Results)
    - **å…³é”®å›¾è¡¨è§£è¯»**ï¼šç‰©ç†é‡çš„ç›¸å…³æ€§ï¼ˆCorrelationsï¼‰æˆ– æ¼”åŒ–è¶‹åŠ¿ã€‚
    - **æ•°å€¼ç»“è®º**ï¼šæå–æ–‡ä¸­çš„æ ¸å¿ƒæ•°å€¼ç»“æœã€‚
    - **æ¨¡å‹éªŒè¯**ï¼šæ¨¡æ‹Ÿæ˜¯å¦é‡ç°äº†è§‚æµ‹ï¼Ÿè§‚æµ‹æ˜¯å¦æ”¯æŒäº†ç†è®ºï¼Ÿ
    
    ### 4. ç»“è®ºä¸è®¨è®º (Conclusions & Discussion)
    - æ ¸å¿ƒç‰©ç†å›¾åƒ (Physical Picture) çš„æ€»ç»“ã€‚
    - å±€é™æ€§ (Caveats) ä¸ ä½œè€…å»ºè®®çš„æœªæ¥å·¥ä½œ (Future Work)ã€‚
    """

    user_prompt = f"è®ºæ–‡æ ‡é¢˜ï¼š{title}\n\nè®ºæ–‡æ­£æ–‡å†…å®¹ï¼š\n{truncated_content}"

    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.2, # ä¿æŒä½æ¸©ï¼Œç¡®ä¿äº‹å®å‡†ç¡®
            stream=False
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"AI æ¥å£è°ƒç”¨å‡ºé”™: {e}"

def clean_filename(title):
    return re.sub(r'[\\/*?:"<>|]', "_", title).strip()

def main():
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)

    if not os.path.exists(INPUT_FILE):
        # å¦‚æœæ²¡æœ‰æ–‡ä»¶ï¼Œè‡ªåŠ¨åˆ›å»ºä¸€ä¸ªç¤ºä¾‹
        with open(INPUT_FILE, "w") as f:
            f.write("# åœ¨è¿™é‡Œç²˜è´´ ArXiv é“¾æ¥ï¼Œä¸€è¡Œä¸€ä¸ª\n")
        print(f"âŒ æ‰¾ä¸åˆ° {INPUT_FILE}ï¼Œå·²è‡ªåŠ¨åˆ›å»ºã€‚è¯·å¡«å…¥é“¾æ¥åé‡è¯•ã€‚")
        return

    with open(INPUT_FILE, 'r', encoding='utf-8') as f:
        urls = [line.strip() for line in f if line.strip() and not line.startswith("#")]

    total = len(urls)
    print(f"ğŸ“‹ å‘ç° {total} ç¯‡è®ºæ–‡å¾…å¤„ç†...\n")

    for i, url in enumerate(urls):
        print(f"--- å¤„ç†ç¬¬ {i+1}/{total} ç¯‡ ---")
        try:
            arxiv_id = extract_arxiv_id(url)
            if not arxiv_id:
                print(f"âš ï¸ è·³è¿‡æ— æ•ˆé“¾æ¥: {url}")
                continue
            
            # 1. æ£€æŸ¥æ˜¯å¦å­˜åœ¨
            temp_search = arxiv.Search(id_list=[arxiv_id])
            try:
                temp_title = next(temp_search.results()).title
            except:
                temp_title = arxiv_id 
                
            safe_title = clean_filename(temp_title)
            output_path = os.path.join(OUTPUT_DIR, f"{safe_title}.md")
            
            if os.path.exists(output_path):
                print(f"âœ… ç¬”è®°å·²å­˜åœ¨ï¼Œè·³è¿‡: {safe_title}")
                continue

            # 2. è·å–å¹¶æ¸…æ´—å†…å®¹
            title, content = get_paper_content(arxiv_id)
            
            # 3. AI åˆ†æ
            report = analyze_with_deepseek(title, content)
            
            # 4. ä¿å­˜
            with open(output_path, "w", encoding="utf-8") as f:
                header = f"# {title}\n\n**ArXiv ID**: [{arxiv_id}]({url})\n**Date**: {time.strftime('%Y-%m-%d')}\n\n---\n\n"
                f.write(header + report)
            
            print(f"âœ… æŠ¥å‘Šå·²ç”Ÿæˆ: {output_path}")
            
        except Exception as e:
            print(f"âŒ å¤„ç†å‡ºé”™ {url}: {e}")
        
        print("\n")

if __name__ == "__main__":
    main()
